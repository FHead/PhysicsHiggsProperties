[CRAB]

jobtype = cmssw
scheduler = remoteGlidein
use_server = 0


[CMSSW]

datasetpath=none
pset=cmssw.py
total_number_of_events=100000
number_of_jobs = 160
increment_seeds = program
output_file = Result.txt

[USER]

return_data = 0

copy_data = 1
#storage_element = t3-srm.ultralight.org
storage_element = cit-se.ultralight.org
storage_path = /srm/v2/server?SFN=/mnt/hadoop
user_remote_dir = /store/user/yichen/13151_SeedCalculation/11351_Run1/__SUBDIR__

publish_data=0
publish_data_name = name_you_prefer
#dbs_url_for_publication = https://cmsdbsprod.cern.ch:8443/cms_dbs_caf_analysis_01_writer/servlet/DBSServlet


[GRID]
rb = CERN

se_black_list = T0,T2_EE_Estonia,T3_MX_Cinvestav,T2_GR_Ioannina
#se_white_list =

#ce_black_list = T2_EE_Estonia
#ce_white_list = T2_US_Caltech

[CONDORG]

# Set this to condor to override the batchsystem defined in gridcat.
#batchsystem = condor

# Specify addition condor_g requirments
# use this requirment to run on a cms dedicated hardare
# globus_rsl = (condor_submit=(requirements 'ClusterName == \"CMS\" && (Arch == \"INTEL\" || Arch == \"X86_64\")'))
# use this requirement to run on the new hardware
#globus_rsl = (condor_submit=(requirements 'regexp(\"cms-*\",Machine)'))

